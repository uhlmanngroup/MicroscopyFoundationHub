# DINO-EM-PEFT
Parameter-efficient fine-tuning (PEFT) of DINOv2 ViT for electron microscopy (EM) segmentation using LoRA.

> **New:** automated reporting and visualization
>
> - `scripts/summarize_seg_results.py` collects per-run metrics into `summary/*.csv`.
> - `scripts/plot_seg_summary.py` turns those CSVs into polished bar/box/line plots plus an optional interactive dashboard. See [Visualizing results](#visualizing-results) below.

## TL;DR

```bash
# 0) create env & install
python -m venv .venv && source .venv/bin/activate
pip install -e .

# 1) compose datasets into one train/test (runs with no args; edit paths in the script header if needed)
python scripts/compose_em_datasets.py

# 2) train (Mac / local)
python scripts/train_em_seg.py --cfg config/lucchi_lora_mac.yaml

# 3) train (CUDA / cluster, Lucchi++)
python scripts/train_em_seg.py --cfg config/lucchi_cluster.yaml
```

To turn raw run folders into publication-ready plots:

```bash
# 4) summarize & plot (assume your runs live under /data/DINO-LoRA/seg)
python scripts/summarize_seg_results.py --root /data/DINO-LoRA/seg
python scripts/plot_seg_summary.py \
  --summary-dir ~/Downloads/summary \
  --output-dir plots/latest \
  --interactive-html plots/latest/dashboard.html
```

## Results Layout & Experiment IDs

All training, evaluation, and feature/analysis scripts share a single results layout driven by the YAML config. Each config must define:

- `experiment_id`: follow `YYYY-MM-DD_SUBTASK_DATASETS_BACKBONE_LORA_TASK` (e.g. `2025-11-20_A1_lucchi+droso_dinov2-base_lora-none_seg`). Update this string before every new run so directories and config copies stay unique.
- `results_root`: the absolute (or `~`-expanded) directory where you keep all run outputs, e.g. `/Users/cfuste/Documents/Results/DINO-LoRA`.
- `task_type`: `"seg"` for segmentation training/eval (`train_em_seg.py`, `eval_em_seg.py`) and `"feats"` for feature extraction/PCA/latent scripts (`extract_features.py`, `run_pca.py`, future FID/LR scripts).

When one of the scripts starts, it creates `<results_root>/<task_type>/<experiment_id>/` and saves:

```
seg/
  <RunID>/
    config_used.yaml      # exact config snapshot
    run_info.txt          # timestamp, git hash, device, img size…
    metrics.json          # sections: train, eval
    logs/
    ckpts/best_model.pt, last_model.pt
    figs/previews/, figs/eval_previews/

feats/
  <RunID>/
    config_used.yaml
    run_info.txt
    metrics.json          # sections: features, pca, etc.
    features.npz          # feature extractor output
    plots/*.png           # PCA / UMAP or other analysis figures
```

`metrics.json` is automatically updated per phase: training adds best-val stats, `eval_em_seg.py` appends Lucchi/Droso IoU/Dice summaries, `extract_features.py` records feature dimensionality, and `run_pca.py` records PCA/UMAP configuration. All scripts keep writing MLflow artifacts as before.

To launch a new experiment:

1. Pick/clone the most relevant YAML under `config/`.
2. Edit the dataset paths as usual, then set a fresh `experiment_id`, ensure `results_root` points to your preferred root folder, and set `task_type` (`"seg"` or `"feats"`).
3. Run the desired script. Outputs, checkpoints, configs, and plots will land under the corresponding run directory so you can diff or archive them safely.

## What's here
* **Backbone**: DINOv2 ViT (`vits14`/`vitb14` etc.) via `torch.hub`.
* **PEFT**: LoRA injected into attention `qkv` and `proj` linear layers; backbone params frozen; only LoRA + segmentation head train.
* **Head**: simple 1×1 conv projection → upsample to image size.
* **Datasets**: utility to compose Drosophila + Lucchi++ into a unified layout.
* **Devices**: `device: "auto"` picks cuda → mps → cpu.

## Datasets
Lucchi, A., Smith, K., Achanta, R., Knott, G., & Fua, P. (2011). Supervoxel-based segmentation of mitochondria in em image stacks with learned shape features. IEEE transactions on medical imaging, 31(2), 474-486. Download [here](https://casser.io/connectomics).

Casser, V., Kang, K., Pfister, H., & Haehn, D. (2020, September). Fast mitochondria detection for connectomics. In Medical Imaging with Deep Learning (pp. 111-120). PMLR. Download [here](https://github.com/unidesigner/groundtruth-drosophila-vnc/tree/master).

For usability purposes, the two dataset are composed into:
````
<BASE>/composed-dinopeft/
  train/images, train/masks
  test/images,  test/masks
  mapping.csv
```
With an 85% split for the Casser et al. dataset. 

To do so, download the original datasets and run:
```bash
python scripts/compose_em_datasets.py
```

## Visualizing results

The `plot_seg_summary.py` helper reads `summary.csv` and `run_metrics.csv` (generated by `summarize_seg_results.py`) and produces bar charts with error bars, run-level box+strip plots, and replicate trends—each sharing a consistent 0–1 y-axis to aid comparisons.

```bash
# Install plotting extras inside your env
pip install matplotlib seaborn pandas plotly

# Summarize runs (omit --root if your results already have summary/* exported)
python scripts/summarize_seg_results.py --root /data/DINO-LoRA/seg

# Generate static PNGs and an optional interactive dashboard
python scripts/plot_seg_summary.py \
  --summary-dir /data/DINO-LoRA/seg/summary \
  --output-dir plots/summary_plots \
  --interactive-html plots/summary_plots/dashboard.html
```

- **Static figures** land inside the output directory and carry descriptive filenames (e.g., `summary_foreground_iou.png`, `runs_mean_iou.png`).
- **Interactive dashboard** (optional) bundles grouped bars, run-level boxes, and replicate trends into a single HTML file powered by Plotly for quick hover-tooltips.
- **Consistent styling**: y-axis limits default to `[0, 1]`, ensuring side-by-side plots remain directly comparable across datasets and LoRA settings.

### Sharing plots in the README

Curate your favorite figures by copying them into `docs/media/` (this repository now ships with that folder and a `.gitkeep` placeholder). To embed them in Markdown, drop an image link such as:

```markdown
![Foreground IoU across DINO sizes](docs/media/foreground_iou.png)
```

Keeping assets in `docs/media/` keeps the repository tidy and avoids broken links when publishing on GitHub. Re-run `plot_seg_summary.py` whenever you collect new sweeps and update the saved figures as needed.

## Acknowledgements
This project stands on the shoulders of excellent open-source work and research. We’re grateful to the authors and maintainers of the following projects and papers:

- **DINOv2 (Meta AI / Facebook Research)**  
  We use DINOv2 Vision Transformers and public pretraining weights (loaded via `torch.hub`) as our frozen backbone. DINOv2 provides strong, general-purpose visual representations that we adapt to electron microscopy via parameter-efficient fine-tuning (PEFT).  
  Repo: <https://github.com/facebookresearch/dinov2>

- **RobvanGastel/dinov3-finetune**  
  This repository informed practical design choices for LoRA-based finetuning of DINOv2/DINOv3 encoders: which linear layers to target (e.g., attention `qkv` and `proj`), how to organize training/evaluation code, and how to integrate PEFT cleanly around a frozen backbone.  
  Repo: <https://github.com/RobvanGastel/dinov3-finetune>

- **samar-khanna/ExPLoRA**  
  ExPLoRA provides a strong reference for adapting Vision Transformers under domain shift via extended pre-training. While our current baseline is supervised LoRA on DINOv2, ExPLoRA guides our roadmap toward semi/self-supervised adaptation on unlabeled EM volumes and domain-shifted pretraining strategies.  
  Repo: <https://github.com/samar-khanna/ExPLoRA>

- **DINOSim (Electron Microscopy zero-shot evaluation)**  
  DINOSim motivates our evaluation focus: it investigates zero-shot detection/segmentation on EM imagery using DINO features and highlights the domain gap for microscopy. We build on that insight by demonstrating how PEFT (LoRA) improves downstream EM segmentation compared to zero-shot.  
  Project/Paper: *DINOSim: Zero-Shot Object Detection and Semantic Segmentation on Electron Microscopy Images.*

**Licensing note:**  
Please review and respect the licenses of upstream repositories (DINOv2, `dinov3-finetune`, ExPLoRA) and any datasets you use. Their terms apply to model weights, code, and data redistributed or fine-tuned within this project.
