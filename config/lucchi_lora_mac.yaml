# Environment: Mac / local dev â€” Lucchi++ dataset

# Composed dataset (train/test already prepared)
train_img_dir: "/Users/cfuste/Documents/Data/ElectronMicroscopy/composed-dinopeft/train/images"
train_mask_dir: "/Users/cfuste/Documents/Data/ElectronMicroscopy/composed-dinopeft/train/masks"
test_img_dir:   "/Users/cfuste/Documents/Data/ElectronMicroscopy/composed-dinopeft/test/images"
test_mask_dir:  "/Users/cfuste/Documents/Data/ElectronMicroscopy/composed-dinopeft/test/masks"

# Switch to Lucchi-only by updating the dataset block + dirs below
# train_img_dir: "/Users/cfuste/Documents/Data/ElectronMicroscopy/Lucchi++/Train_In"
# train_mask_dir: "/Users/cfuste/Documents/Data/ElectronMicroscopy/Lucchi++/Train_Out"
# test_img_dir:   "/Users/cfuste/Documents/Data/ElectronMicroscopy/Lucchi++/Test_In"
# test_mask_dir:  "/Users/cfuste/Documents/Data/ElectronMicroscopy/Lucchi++/Test_Out"

# results + experiment naming
# Update experiment_id for every run following YYYY-MM-DD_SUBTASK_DATASETS_BACKBONE_LORA_TASK
experiment_id: "2025-01-01_A1_lucchi-droso_dinov2-small_lora-r8_seg"
results_root: "/Users/cfuste/Documents/Results/DINO-LoRA"
task_type: "seg"

# ---- model / backbone ----
dino_size: small
img_size:
  mode: longest_edge
  target: 1022   # largest <= 1024 that stays multiple of 14
  patch_multiple: 14
  rounding: floor
num_classes: 2

dataset:
  type: paired        # change to "lucchi" when pointing to Lucchi++ dirs
  params:
    pair_mode: "stem"
    recursive: false

# ---- MLflow ----
mlflow_experiment_name: 01_A1_lucchi-droso_dinov2-small_lora-r8_seg

# ---- LoRA ----
use_lora: true
lora_targets: [attn.qkv, attn.proj]
lora_rank: 8
lora_alpha: 32

# ---- training ----
batch_size: 1
epochs: 1
lr: 0.00005
weight_decay: 0.0001
num_workers: 8
device: auto
amp: true            # first run: stability check without AMP

# ---- loss (CE + Tversky on foreground) ----
loss: dice      # ce, dice, ce_tversky 
class_weights: [1.0, 15.0]    # bg, fg
tversky_alpha: 0.7
tversky_beta: 0.3
tversky_eps: 1.0e-6
tversky_weight: 0.6   # lambda: 0.6*Tversky + 0.4*CE

# ---- training safety ----
clip_grad_norm: 1.0   # global grad clip

# ---- export / viz (kept; argmax is used for CE path) ----
binarize: true
binarize_threshold: 128
